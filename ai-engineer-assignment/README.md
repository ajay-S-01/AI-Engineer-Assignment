# AI Engineer Assignment

A comprehensive AI pipeline that combines weather information retrieval and RAG (Retrieval-Augmented Generation) capabilities using LangGraph for workflow orchestration.

## Features

- ğŸŒ¤ï¸ **Weather Integration**: Get real-time weather information for any city
- ğŸ“š **RAG Pipeline**: Ask questions about uploaded PDF documents
- ğŸ”„ **LangGraph Workflow**: Intelligent routing between weather and RAG based on query type
- ğŸ¯ **Multiple UIs**: Streamlit app, React frontend, and FastAPI backend
- ğŸ”§ **Modular Design**: Clean, extensible architecture
- ğŸ³ **Docker Ready**: Easy deployment with Docker Compose
- ğŸš€ **Production Ready**: FastAPI backend with CORS, React frontend with TypeScript

## Architecture

The system uses LangGraph to create a workflow that intelligently routes queries:

```
START â†’ Decision Node â†’ [Weather Node | RAG Node] â†’ END
```

- **Decision Node**: Analyzes queries to determine if they're weather-related or document-related
- **Weather Node**: Handles weather queries using OpenWeather API
- **RAG Node**: Processes document questions using retrieval-augmented generation

## Installation

### Prerequisites

- Python 3.8+
- pip or poetry

### Setup

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd ai-engineer-assignment
   ```

2. **Install dependencies**
   ```bash
   pip install -e .
   # or for development
   pip install -e ".[dev]"
   ```

3. **Set up environment variables**
   ```bash
   cp env.example .env
   # Edit .env with your API keys
   ```

4. **Required API Keys**
   - OpenWeather API key (for weather functionality)
   - Gemini API key OR OpenAI API key (for LLM functionality)

## Configuration

Create a `.env` file with the following variables:

```env
# Weather API
OPENWEATHER_API_KEY=your_openweather_api_key_here

# LLM Configuration
LLM_PROVIDER=gemini  # Options: "gemini" or "openai"

# Gemini Configuration
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-2.5-flash

# OpenAI Configuration (if using OpenAI)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini

# File Paths
PDF_PATH=data/sample.pdf
FAISS_INDEX_PATH=faiss_index

# Embedding Model
EMBEDDING_MODEL=all-MiniLM-L6-v2
```

## Usage

### Option 1: Docker Compose (Recommended)

```bash
# Start all services
docker-compose up --build

# Access the application
# Frontend: http://localhost:3000
# Backend API: http://localhost:8000
# API Docs: http://localhost:8000/docs
```

### Option 2: Development Mode

#### Start Backend and Frontend Together
```bash
# Windows
scripts/start-dev.bat

# Linux/Mac
chmod +x scripts/start-dev.sh
./scripts/start-dev.sh
```

#### Start Services Individually

**Backend (FastAPI):**
```bash
cd backend
pip install -r requirements.txt
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

**Frontend (React):**
```bash
cd frontend
npm install
npm start
```

**Streamlit App (Alternative UI):**
```bash
streamlit run src/app.py
```

### Option 3: Using the API Programmatically

```python
import requests

# Query the AI pipeline
response = requests.post("http://localhost:8000/api/v1/query/query", 
                        json={"query": "What's the weather in London?"})
print(response.json())

# Weather-specific endpoint
response = requests.post("http://localhost:8000/api/v1/weather/weather",
                        json={"city": "London"})
print(response.json())

# RAG-specific endpoint
response = requests.post("http://localhost:8000/api/v1/rag/rag",
                        json={"question": "What is RAG?"})
print(response.json())
```

## Project Structure

```
ai-engineer-assignment/
â”œâ”€â”€ backend/               # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/          # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ endpoints/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ health.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ query.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ weather.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ rag.py
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ core/         # Core functionality
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”‚   â”œâ”€â”€ cors.py
â”‚   â”‚   â”‚   â”œâ”€â”€ dependencies.py
â”‚   â”‚   â”‚   â””â”€â”€ models.py
â”‚   â”‚   â”œâ”€â”€ main.py       # FastAPI app
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/              # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/   # React components
â”‚   â”‚   â”œâ”€â”€ hooks/        # Custom hooks
â”‚   â”‚   â”œâ”€â”€ services/     # API services
â”‚   â”‚   â”œâ”€â”€ types/        # TypeScript types
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â””â”€â”€ index.tsx
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ nginx.conf
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ src/                   # Original source code (Streamlit)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py            # Streamlit application
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”œâ”€â”€ langgraph_engine.py
â”‚   â”œâ”€â”€ llm_wrappers.py
â”‚   â”œâ”€â”€ rag_chain.py
â”‚   â”œâ”€â”€ vectorstore.py
â”‚   â””â”€â”€ weather.py
â”œâ”€â”€ tests/                 # Test suite
â”œâ”€â”€ docs/                  # Documentation
â”œâ”€â”€ scripts/               # Development scripts
â”œâ”€â”€ data/                  # Data files
â”œâ”€â”€ faiss_index/           # FAISS index storage
â”œâ”€â”€ docker-compose.yml     # Docker orchestration
â”œâ”€â”€ nginx.conf            # Nginx configuration
â”œâ”€â”€ .gitignore
â”œâ”€â”€ env.example
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Development

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src

# Run specific test categories
pytest -m unit          # Unit tests only
pytest -m integration   # Integration tests only
pytest -m "not slow"    # Skip slow tests
```

### Code Quality

```bash
# Format code
black src tests
isort src tests

# Lint code
flake8 src tests
mypy src

# Run all quality checks
pre-commit run --all-files
```

### Adding New Features

1. **New API Endpoints**: Add new endpoints in `backend/app/api/endpoints/`
2. **New React Components**: Add components in `frontend/src/components/`
3. **New LangGraph Nodes**: Add new processing nodes to `src/langgraph_engine.py`
4. **New Data Sources**: Extend the data loading in `src/data_loader.py`
5. **New LLM Providers**: Add wrappers in `src/llm_wrappers.py`

## API Reference

### Backend API Endpoints

- `POST /api/v1/query/query` - Main query endpoint (weather or RAG)
- `POST /api/v1/weather/weather` - Weather-specific endpoint
- `GET /api/v1/weather/weather/{city}` - Get weather by city
- `POST /api/v1/rag/rag` - RAG-specific endpoint
- `GET /api/v1/health/health` - Health check

### Core Classes

- `LangGraphEngine`: Main workflow orchestrator
- `Settings`: Configuration management
- `GraphState`: LangGraph state definition

### Key Functions

- `build_rag_chain()`: Creates RAG processing chain
- `get_weather_for_city()`: Fetches weather data
- `build_faiss_from_docs()`: Creates vector store from documents

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Ensure all tests pass
6. Submit a pull request

## License

MIT License - see LICENSE file for details.

## Troubleshooting

### Common Issues

1. **Import Errors**: Ensure you're running from the project root and have installed the package
2. **API Key Errors**: Verify your `.env` file has the correct API keys
3. **PDF Loading Issues**: Ensure the PDF file exists and is not corrupted
4. **Memory Issues**: For large PDFs, consider using a smaller embedding model

### Getting Help

- Check the test files for usage examples
- Review the configuration in `config.py`
- Ensure all dependencies are installed correctly
